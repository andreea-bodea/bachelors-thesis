{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0774a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# read json file as dataframe\n",
    "parler_df_1 = pd.read_json('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000001.ndjson', lines = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa2badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comments', 'body', 'bodywithurls', 'createdAt', 'createdAtformatted',\n",
      "       'creator', 'datatype', 'depth', 'depthRaw', 'followers', 'following',\n",
      "       'hashtags', 'id', 'lastseents', 'links', 'media', 'posts', 'sensitive',\n",
      "       'shareLink', 'upvotes', 'urls', 'username', 'verified', 'article',\n",
      "       'impressions', 'preview', 'reposts', 'state', 'parent', 'color',\n",
      "       'commentDepth', 'controversy', 'downvotes', 'post', 'score',\n",
      "       'isPrimary', 'conversation', 'replyingTo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(parler_df_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab231d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comments', 'body', 'bodywithurls', 'createdAt', 'createdAtformatted',\n",
      "       'creator', 'datatype', 'depth', 'depthRaw', 'followers', 'following',\n",
      "       'hashtags', 'id', 'lastseents', 'links', 'media', 'posts', 'sensitive',\n",
      "       'shareLink', 'upvotes', 'urls', 'username', 'verified', 'article',\n",
      "       'impressions', 'preview', 'reposts', 'state', 'parent', 'color',\n",
      "       'commentDepth', 'controversy', 'downvotes', 'post', 'score',\n",
      "       'isPrimary', 'conversation', 'replyingTo'],\n",
      "      dtype='object')\n",
      "Dimension of whole dataframe: (1095287, 38)\n",
      "Index(['body'], dtype='object')\n",
      "Dimension of final dataframe: (1095287, 1)\n"
     ]
    }
   ],
   "source": [
    "# remove columns  \n",
    "\n",
    "print(parler_df_1.columns)\n",
    "print('Dimension of whole dataframe: ' + str(parler_df_1.shape)) # df.shape -> (rows, columns)\n",
    "\n",
    "# final_df_1 = pd.DataFrame()\n",
    "# final_df_1['body'] = df['body'].copy()\n",
    "# final_df_1['createdAtformatted'] = df['createdAtformatted'].copy()\n",
    "parler_df_1.drop(parler_df_1.iloc[:, 2:38], inplace = True, axis = 1) # remove all columns between column index 2 to 38\n",
    "parler_df_1.drop(['comments'], inplace = True, axis = 1) # remove first column\n",
    "\n",
    "print(parler_df_1.columns)\n",
    "print('Dimension of final dataframe: ' + str(parler_df_1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abe49de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of final whole dataframe: (1095287, 1)\n",
      "0          ‚Ä™ CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "1                                                           \n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "                                 ...                        \n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095284                                                     \n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "1095286                                                     \n",
      "Name: body, Length: 1095287, dtype: object\n",
      "Dimension of final dataframe after preprocessing: (636482, 1)\n",
      "0          ‚Ä™ CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "5          Hillary Clinton Calls Bernie Sanders a Sore Lo...\n",
      "                                 ...                        \n",
      "1095275    SHE WANTS ANOTHER BENGHAZI and TO MURDER MORE ...\n",
      "1095281                       üá∫üá∏‚ù§Ô∏èüá∫üá∏‚ù§Ô∏èüôèüèª #trump2020landslide\n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# filtering out null values\n",
    "\n",
    "print('Dimension of final whole dataframe: ' + str(parler_df_1.shape)) \n",
    "print(parler_df_1['body'])\n",
    "\n",
    "parler_df_1['body'].replace(\"\", np.nan, inplace=True)\n",
    "parler_df_1.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "print('Dimension of final dataframe after preprocessing: ' + str(parler_df_1.shape)) \n",
    "\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93089dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea913a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Prerequisites ‚Äì Download nltk stopwords and spacy model\n",
    "\n",
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915e81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Import Packages\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "import pyLDAvis.gensim # !!! name changed from gensim to gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702919b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prepare Stopwords\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92eb6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u202a CHILLS! I loved hearing Airforce Technical SGT. Nalani Quintello '\n",
      " 'singing the National Anthem in front of President Trump & First Lady, '\n",
      " 'Melania Trump & all the fans at #DAYTONA500 A huge shout out to the amazing '\n",
      " 'Thunderbirds for the fly over! This is the America I Loveüá∫üá∏ ECHO if you feel '\n",
      " 'the same! \\u202c']\n"
     ]
    }
   ],
   "source": [
    "# 7. Remove emails and newline characters\n",
    "\n",
    "# Convert to list\n",
    "data = parler_df_1.body.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95db0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chills', 'loved', 'hearing', 'airforce', 'technical', 'sgt', 'nalani', 'quintello', 'singing', 'the', 'national', 'anthem', 'in', 'front', 'of', 'president', 'trump', 'first', 'lady', 'melania', 'trump', 'all', 'the', 'fans', 'at', 'daytona', 'huge', 'shout', 'out', 'to', 'the', 'amazing', 'thunderbirds', 'for', 'the', 'fly', 'over', 'this', 'is', 'the', 'america', 'love', 'echo', 'if', 'you', 'feel', 'the', 'same']]\n"
     ]
    }
   ],
   "source": [
    "# 8. Tokenize words and Clean-up text\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05fc7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chills', 'loved', 'hearing', 'airforce', 'technical', 'sgt', 'nalani', 'quintello', 'singing', 'the', 'national_anthem', 'in', 'front', 'of', 'president', 'trump', 'first', 'lady_melania', 'trump', 'all', 'the', 'fans', 'at', 'daytona', 'huge', 'shout', 'out', 'to', 'the', 'amazing', 'thunderbirds', 'for', 'the', 'fly', 'over', 'this', 'is', 'the', 'america', 'love', 'echo', 'if', 'you', 'feel', 'the', 'same']\n"
     ]
    }
   ],
   "source": [
    "# 9. Creating Bigram and Trigram Models\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd4ef604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chill', 'love', 'hearing', 'airforce', 'technical', 'nalani', 'quintello', 'singe', 'front', 'trump', 'fan', 'daytona', 'huge', 'shout', 'amazing', 'thunderbird', 'fly', 'feel']]\n"
     ]
    }
   ],
   "source": [
    "# 10. Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en \n",
    "# !!! As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full pipeline package name 'en_core_web_sm' instead.\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dfee3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('airforce', 1),\n",
       "  ('amazing', 1),\n",
       "  ('chill', 1),\n",
       "  ('daytona', 1),\n",
       "  ('fan', 1),\n",
       "  ('feel', 1),\n",
       "  ('fly', 1),\n",
       "  ('front', 1),\n",
       "  ('hearing', 1),\n",
       "  ('huge', 1),\n",
       "  ('love', 1),\n",
       "  ('nalani', 1),\n",
       "  ('quintello', 1),\n",
       "  ('shout', 1),\n",
       "  ('singe', 1),\n",
       "  ('technical', 1),\n",
       "  ('thunderbird', 1),\n",
       "  ('trump', 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "id2word[0]\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66fdf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56dcee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Building the Topic Model\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# Build LDA model\n",
    "@jit\n",
    "def LDA():\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e7ec527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adela\\AppData\\Local\\Temp/ipykernel_23204/465458224.py:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"LDA\" failed type inference due to: \u001b[1mUntyped global name 'id2word':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'gensim.corpora.dictionary.Dictionary'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\Users\\adela\\AppData\\Local\\Temp\\ipykernel_23204\\465458224.py\", line 8:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\ANDREEA\\anaconda3\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"LDA\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\Users\\adela\\AppData\\Local\\Temp\\ipykernel_23204\\465458224.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\ANDREEA\\anaconda3\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\Users\\adela\\AppData\\Local\\Temp\\ipykernel_23204\\465458224.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d05da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba869919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Visualize the topics-keywords\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word) # gensim_models instead od gensim https://github.com/bmabey/pyLDAvis/issues/131\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b7429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad119fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2bd8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Essentials\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datapane as dp\n",
    "#dp.login(token='INSERT_TOKEN_HERE')\n",
    "# Gensim and LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "# NLP stuff\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "# Plotting tools\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Miscellaneous\n",
    "from sklearn.manifold import TSNE\n",
    "from pprint import pprint\n",
    "\n",
    "def preprocess(text_col):\n",
    "    \"\"\"This function will apply NLP preprocessing lambda functions over a pandas series such as df['text'].\n",
    "       These functions include converting text to lowercase, removing emojis, expanding contractions, removing punctuation,\n",
    "       removing numbers, removing stopwords, lemmatization, etc.\"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # remove emojis\n",
    "    text_col = text_col.apply(lambda x: demoji.replace(x, \"\"))\n",
    "    \n",
    "    # expand contractions  \n",
    "    text_col = text_col.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    text_col = text_col.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove numbers\n",
    "    text_col = text_col.apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = [sw for sw in nltk.corpus.stopwords.words('english') if sw not in ['not', 'no']]\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "\n",
    "    # lemmatization\n",
    "    text_col = text_col.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "    # remove short words\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "    return text_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be1426b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          ‚Ä™ CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "5          Hillary Clinton Calls Bernie Sanders a Sore Lo...\n",
      "                                 ...                        \n",
      "1095275    SHE WANTS ANOTHER BENGHAZI and TO MURDER MORE ...\n",
      "1095281                       üá∫üá∏‚ù§Ô∏èüá∫üá∏‚ù§Ô∏èüôèüèª #trump2020landslide\n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23204/4131165679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23204/2529739588.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text_col)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# remove emojis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mtext_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdemoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# expand contractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANDREEA\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32mC:\\ANDREEA\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANDREEA\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANDREEA\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23204/2529739588.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# remove emojis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mtext_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdemoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# expand contractions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ANDREEA\\anaconda3\\lib\\site-packages\\demoji\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mset_emoji_pattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(parler_df_1['body'])\n",
    "\n",
    "preprocess(parler_df_1['body'])\n",
    "\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea986c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a286483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd25201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
