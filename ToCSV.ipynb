{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0774a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reading, preprocessing and saving as csv\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# read json file as dataframe\n",
    "parler_df = pd.read_json('E:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000000.ndjson', lines = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d62b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parler_df['body'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82488b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['body'], dtype='object')\n",
      "Dimension of whole dataframe: (1094836, 1)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['comments'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f4a0ca66f4f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# final_df_1['createdAtformatted'] = df['createdAtformatted'].copy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m38\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove all columns between column index 2 to 38\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove first column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4307\u001b[0m         \"\"\"\n\u001b[1;32m-> 4308\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Apps\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['comments'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# remove columns  \n",
    "\n",
    "print(parler_df.columns)\n",
    "print('Dimension of whole dataframe: ' + str(parler_df.shape) + '\\n') # df.shape -> (rows, columns)\n",
    "\n",
    "# final_df_1 = pd.DataFrame()\n",
    "# final_df_1['body'] = df['body'].copy()\n",
    "# final_df_1['createdAtformatted'] = df['createdAtformatted'].copy()\n",
    "parler_df.drop(parler_df.iloc[:, 2:38], inplace = True, axis = 1) # remove all columns between column index 2 to 38\n",
    "parler_df.drop(['comments'], inplace = True, axis = 1) # remove first column\n",
    "\n",
    "print(parler_df.columns)\n",
    "print('Dimension of final dataframe: ' + str(parler_df.shape) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abe49de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of final whole dataframe: (1095287, 1)\n",
      "0          â€ª CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "1                                                           \n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "                                 ...                        \n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095284                                                     \n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "1095286                                                     \n",
      "Name: body, Length: 1095287, dtype: object\n",
      "Dimension of final dataframe after preprocessing: (636482, 1)\n",
      "0          â€ª CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "5          Hillary Clinton Calls Bernie Sanders a Sore Lo...\n",
      "                                 ...                        \n",
      "1095275    SHE WANTS ANOTHER BENGHAZI and TO MURDER MORE ...\n",
      "1095281                       ðŸ‡ºðŸ‡¸â¤ï¸ðŸ‡ºðŸ‡¸â¤ï¸ðŸ™ðŸ» #trump2020landslide\n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# filter out null values\n",
    "\n",
    "print('Dimension of final dataframe: ' + str(parler_df.shape)) \n",
    "print(parler_df['body'] + '\\n')\n",
    "\n",
    "parler_df['body'].replace(\"\", np.nan, inplace=True)\n",
    "parler_df.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "print('Dimension of final dataframe after preprocessing: ' + str(parler_df.shape)) \n",
    "print(parler_df['body'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eaab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          â€ª CHILLS! \\n\\nI loved hearing Airforce Technic...\n",
      "2          Justice Department Zeroes In On Cuomo's COVID ...\n",
      "3          Interesting how ultra-liberal, hysterical, Hol...\n",
      "4          Petraeus Says Trump May Have Restored U.S. 'De...\n",
      "5          Hillary Clinton Calls Bernie Sanders a Sore Lo...\n",
      "                                 ...                        \n",
      "1095275    SHE WANTS ANOTHER BENGHAZI and TO MURDER MORE ...\n",
      "1095281                       ðŸ‡ºðŸ‡¸â¤ï¸ðŸ‡ºðŸ‡¸â¤ï¸ðŸ™ðŸ» #trump2020landslide\n",
      "1095282    I just shared this to \"Chrissy\" Wallace's FB m...\n",
      "1095283                           What have you got to lose?\n",
      "1095285    So... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          â€ª chills! i loved hearing airforce technical s...\n",
      "2          justice department zeroes in on cuomo's covid ...\n",
      "3          interesting how ultra-liberal, hysterical, hol...\n",
      "4          petraeus says trump may have restored u.s. 'de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                       ðŸ‡ºðŸ‡¸â¤ï¸ðŸ‡ºðŸ‡¸â¤ï¸ðŸ™ðŸ» #trump2020landslide\n",
      "1095282    i just shared this to \"chrissy\" wallace's fb m...\n",
      "1095283                           what have you got to lose?\n",
      "1095285    so... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# convert to lowercase\n",
    "\n",
    "print(parler_df_1['body'])\n",
    "\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e1b60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          â€ª chills! i loved hearing airforce technical s...\n",
      "2          justice department zeroes in on cuomo's covid ...\n",
      "3          interesting how ultra-liberal, hysterical, hol...\n",
      "4          petraeus says trump may have restored u.s. 'de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                       ðŸ‡ºðŸ‡¸â¤ï¸ðŸ‡ºðŸ‡¸â¤ï¸ðŸ™ðŸ» #trump2020landslide\n",
      "1095282    i just shared this to \"chrissy\" wallace's fb m...\n",
      "1095283                           what have you got to lose?\n",
      "1095285    so... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          â€ª chills! i loved hearing airforce technical s...\n",
      "2          justice department zeroes in on cuomo's covid ...\n",
      "3          interesting how ultra-liberal, hysterical, hol...\n",
      "4          petraeus says trump may have restored u.s. 'de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                  #trump2020landslide\n",
      "1095282    i just shared this to \"chrissy\" wallace's fb m...\n",
      "1095283                           what have you got to lose?\n",
      "1095285    so... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove emojis\n",
    "\n",
    "import demoji\n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: demoji.replace(x, \"\"))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a73fa177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #trump2020landslide\n"
     ]
    }
   ],
   "source": [
    "print(parler_df_1['body'][1095281])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d15352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored u s dete...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we gonna talk about the fact that one i...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored you s de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we going to talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# expand contractions  \n",
    "\n",
    "import contractions\n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1368a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored u s dete...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we gonna talk about the fact that one i...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored u s dete...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we gonna talk about the fact that one i...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "\n",
    "import string \n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7f77fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          â€ª chills! i loved hearing airforce technical s...\n",
      "2          justice department zeroes in on cuomo's covid ...\n",
      "3          interesting how ultra-liberal, hysterical, hol...\n",
      "4          petraeus says trump may have restored u.s. 'de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                  #trump2020landslide\n",
      "1095282    i just shared this to \"chrissy\" wallace's fb m...\n",
      "1095283                           what have you got to lose?\n",
      "1095285    so... are we gonna talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored u s dete...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we gonna talk about the fact that one i...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove numbers\n",
    "\n",
    "import re\n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3277f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bodea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dce0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          chills i loved hearing airforce technical sgt ...\n",
      "2          justice department zeroes in on cuomo s covid ...\n",
      "3          interesting how ultra liberal hysterical holly...\n",
      "4          petraeus says trump may have restored you s de...\n",
      "5          hillary clinton calls bernie sanders a sore lo...\n",
      "                                 ...                        \n",
      "1095275    she wants another benghazi and to murder more ...\n",
      "1095281                                      trump landslide\n",
      "1095282    i just shared this to chrissy wallace s fb mes...\n",
      "1095283                            what have you got to lose\n",
      "1095285    so are we going to talk about the fact that on...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chills loved hearing airforce technical sgt na...\n",
      "2                justice department zeroes cuomo covid cover\n",
      "3          interesting ultra liberal hysterical hollywood...\n",
      "4          petraeus says trump may restored deterrence ki...\n",
      "5          hillary clinton calls bernie sanders sore lose...\n",
      "                                 ...                        \n",
      "1095275    wants another benghazi murder navy seals outra...\n",
      "1095281                                      trump landslide\n",
      "1095282                  shared chrissy wallace fb messenger\n",
      "1095283                                             got lose\n",
      "1095285    going talk fact one five mail ballots new jers...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "print(parler_df_1['body'])\n",
    "stopwords = [sw for sw in nltk.corpus.stopwords.words('english') if sw not in ['not', 'no']]\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "print(parler_df_1['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d49f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          chills loved hearing airforce technical sgt na...\n",
      "2                justice department zeroes cuomo covid cover\n",
      "3          interesting ultra liberal hysterical hollywood...\n",
      "4          petraeus says trump may restored deterrence ki...\n",
      "5          hillary clinton calls bernie sanders sore lose...\n",
      "                                 ...                        \n",
      "1095275    wants another benghazi murder navy seals outra...\n",
      "1095281                                      trump landslide\n",
      "1095282                  shared chrissy wallace fb messenger\n",
      "1095283                                             got lose\n",
      "1095285    going talk fact one five mail ballots new jers...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chill loved hearing airforce technical sgt nal...\n",
      "2                  justice department zero cuomo covid cover\n",
      "3          interesting ultra liberal hysterical hollywood...\n",
      "4          petraeus say trump may restored deterrence kil...\n",
      "5          hillary clinton call bernie sander sore loser ...\n",
      "                                 ...                        \n",
      "1095275    want another benghazi murder navy seal outrageous\n",
      "1095281                                      trump landslide\n",
      "1095282                  shared chrissy wallace fb messenger\n",
      "1095283                                             got lose\n",
      "1095285    going talk fact one five mail ballot new jerse...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23cb3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          chill loved hearing airforce technical sgt nal...\n",
      "2                  justice department zero cuomo covid cover\n",
      "3          interesting ultra liberal hysterical hollywood...\n",
      "4          petraeus say trump may restored deterrence kil...\n",
      "5          hillary clinton call bernie sander sore loser ...\n",
      "                                 ...                        \n",
      "1095275    want another benghazi murder navy seal outrageous\n",
      "1095281                                      trump landslide\n",
      "1095282                  shared chrissy wallace fb messenger\n",
      "1095283                                             got lose\n",
      "1095285    going talk fact one five mail ballot new jerse...\n",
      "Name: body, Length: 636482, dtype: object\n",
      "0          chill loved hearing airforce technical sgt nal...\n",
      "2                  justice department zero cuomo covid cover\n",
      "3          interesting ultra liberal hysterical hollywood...\n",
      "4          petraeus say trump may restored deterrence kil...\n",
      "5          hillary clinton call bernie sander sore loser ...\n",
      "                                 ...                        \n",
      "1095275    want another benghazi murder navy seal outrageous\n",
      "1095281                                      trump landslide\n",
      "1095282                     shared chrissy wallace messenger\n",
      "1095283                                             got lose\n",
      "1095285    going talk fact one five mail ballot new jerse...\n",
      "Name: body, Length: 636482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove short words\n",
    "\n",
    "print(parler_df_1['body'])\n",
    "parler_df_1['body'] = parler_df_1['body'].apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eaec9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df_1.to_csv('parler_df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93089dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import datapane as dp\n",
    "#dp.login(token='INSERT_TOKEN_HERE')\n",
    "\n",
    "# NLP stuff\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "df.to_csv('out.csv')\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "\n",
    "def preprocess(text_col):\n",
    "    \"\"\"This function will apply NLP preprocessing lambda functions over a pandas series such as df['text'].\n",
    "       These functions include converting text to lowercase, removing emojis, expanding contractions, removing punctuation,\n",
    "       removing numbers, removing stopwords, lemmatization, etc.\"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # remove emojis\n",
    "    text_col = text_col.apply(lambda x: demoji.replace(x, \"\"))\n",
    "    \n",
    "    # expand contractions  \n",
    "    text_col = text_col.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    text_col = text_col.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove numbers\n",
    "    text_col = text_col.apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = [sw for sw in nltk.corpus.stopwords.words('english') if sw not in ['not', 'no']]\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "\n",
    "    # lemmatization\n",
    "    text_col = text_col.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "    # remove short words\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "    return text_col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
