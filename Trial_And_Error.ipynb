{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNRhTlTZb3H",
    "outputId": "e8d04646-f634-4c45-ffe5-0381649f766f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>createdAtformatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>week anarchist threatened destroy emancipation...</td>\n",
       "      <td>2020-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happening now alex jones leading crowd trump s...</td>\n",
       "      <td>2020-11-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that president one time</td>\n",
       "      <td>2019-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>former michigan trump campaign director scott ...</td>\n",
       "      <td>2020-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you all think show like recount genuinely inte...</td>\n",
       "      <td>2020-11-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608940</th>\n",
       "      <td>piada</td>\n",
       "      <td>2020-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608941</th>\n",
       "      <td>breaking rudy giuliani dominion whistle blower...</td>\n",
       "      <td>2020-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608942</th>\n",
       "      <td>imagine trying potus face attack insidious lef...</td>\n",
       "      <td>2020-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608943</th>\n",
       "      <td>together now together now aaaaaaaaaaaaaaaaaaaa...</td>\n",
       "      <td>2020-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608944</th>\n",
       "      <td>hahaha rest piss</td>\n",
       "      <td>2020-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608945 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body createdAtformatted\n",
       "0       week anarchist threatened destroy emancipation...         2020-06-26\n",
       "1       happening now alex jones leading crowd trump s...         2020-11-19\n",
       "2                                 that president one time         2019-06-06\n",
       "3       former michigan trump campaign director scott ...         2020-11-11\n",
       "4       you all think show like recount genuinely inte...         2020-11-21\n",
       "...                                                   ...                ...\n",
       "608940                                              piada         2020-12-09\n",
       "608941  breaking rudy giuliani dominion whistle blower...         2020-11-11\n",
       "608942  imagine trying potus face attack insidious lef...         2020-11-25\n",
       "608943  together now together now aaaaaaaaaaaaaaaaaaaa...         2020-01-12\n",
       "608944                                   hahaha rest piss         2020-12-13\n",
       "\n",
       "[608945 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parler_df = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_020_dates.csv')\n",
    "parler_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KlC1BXGZb3Y"
   },
   "outputs": [],
   "source": [
    "print('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpRBZ3E2Zb3b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "\"\"\"\n",
    "# with open('C:\\ANDREEA\\parler_data000000000000.ndjson', 'r') as datafile:\n",
    "with open('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000000.ndjson', 'r') as datafile:\n",
    "    for line in datafile:\n",
    "        data = json.loads(line)\n",
    "parler_df = pd.DataFrame(data)\n",
    "\n",
    "parler_df_1 = pd.read_json('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000001.ndjson', \n",
    "                         lines=True, encoding='utf-8-sig', encoding_errors='ignore', orient='records')\n",
    "\"\"\"\n",
    "\n",
    "# Convert a JSON string to pandas object.\n",
    "parler_df_0 = pd.read_json('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000000.ndjson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp7I3gf1Zb3g"
   },
   "outputs": [],
   "source": [
    "print(parler_df_0.columns)\n",
    "print(parler_df_0.head(3)) # first 3 rows\n",
    "print(parler_df_0.tail(3)) # last 3 rows\n",
    "print(parler_df_0.dtypes)\n",
    "print(parler_df_0.info)\n",
    "# parler_df_0.shape\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsYTgwfeZb3j"
   },
   "outputs": [],
   "source": [
    "parler_df_1 = pd.read_json('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000001.ndjson', \n",
    "                         lines=True, encoding='utf-8-sig', encoding_errors='ignore', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_oLGp_MZb3l"
   },
   "outputs": [],
   "source": [
    "dfs = [] # an empty list to store the data frames\n",
    "for i in range(1, 2):\n",
    "    path = 'D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data00000000000' + str(i) + '.ndjson'\n",
    "    data = pd.read_json(path, lines=True) # read data frame from json file\n",
    "    print('ndjson file number ' + str(i) + ' with dimension: ')\n",
    "    print(data.shape)\n",
    "    dfs.append(data) # append the data frame to the list\n",
    "\n",
    "parler_data = pd.concat(dfs, ignore_index=True) # concatenate all the dat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdhPWJbNZb3o"
   },
   "outputs": [],
   "source": [
    "# new = old[['A', 'C', 'D']].copy()\n",
    "\n",
    "final_df = parler_data[['comment']].copy()\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz1m3TYWZb3s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezfWIfjeZb3u"
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/nltk_data/\n",
    "# http://universal.elra.info/product_info.php?cPath=42_43&products_id=1509\n",
    "# https://www.nltk.org/book/ch02.html\n",
    "\n",
    "from nltk.corpus import cess_esp\n",
    "spanish_corpus = cess_esp.words() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_lbGqM-Zb3x"
   },
   "outputs": [],
   "source": [
    "parler_df = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_020_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh4LN7VWZb3z"
   },
   "outputs": [],
   "source": [
    "parler_df['body'] = parler_df['body'].apply(lambda x: ' '.join([w.strip() for w in x.split() if (w in cess_esp.words())]))\n",
    "print(parler_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyii-iB_Zb31"
   },
   "outputs": [],
   "source": [
    "df = parler_df[199:203]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ughwbnNgZb33"
   },
   "outputs": [],
   "source": [
    "# df['body'] = df['body'].apply(lambda x: ' '.join([w.strip() for w in x.split() if (w in cess_esp.words())]))\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if i not in spanish_corpus]))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciFMk2OKZb35"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41290028/removing-non-english-words-from-text-using-python\n",
    "\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "# print(words[0:10])\n",
    "\n",
    "df = parler_df[179:203]\n",
    "print(df)\n",
    "# sent = \"boys\"\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if i in words]))\n",
    "#\" \".join(w for w in nltk.wordpunct_tokenize(df['body']) \\\n",
    " #        if w.lower() in words )\n",
    "# 'Io to the beach with my'\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi3IGscEZb38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tb2GcQAEZb3-"
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator, constants\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI26nIDQZb3_"
   },
   "outputs": [],
   "source": [
    "# init the Google API translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTuEpXG1Zb4C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translation = translator.translate(\"Hola Mundo\")\n",
    "print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-E_c49gZb4D"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Hello everyone\",\n",
    "    \"How are you ?\",\n",
    "    \"Do you speak english ?\",\n",
    "    \"Good bye!\"\n",
    "]\n",
    "translations = translator.translate(sentences, dest=\"tr\")\n",
    "for translation in translations:\n",
    "    print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPkwPxW6Zb4E"
   },
   "outputs": [],
   "source": [
    "detection = translator.detect(\"english\")\n",
    "print(\"Language code:\", detection.lang)\n",
    "print(\"Confidence:\", detection.confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm8swM3vZb4G"
   },
   "outputs": [],
   "source": [
    "if x in df.body\n",
    "\n",
    "translations = translator.translate(df.body, dest=\"en\")\n",
    "print(translations) \n",
    "# for translation in translations:\n",
    "#   print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gvc6JhyCZb4H",
    "outputId": "8fc1148f-8255-4f8a-c433-5c5ffc547889"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"lid.176.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUk9WgaUZb4I",
    "outputId": "6fae2f04-815e-480a-d67e-0cf45d7fddce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a1b7523cd5b5>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"language\"] = df[\"body\"].apply(fast_detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368    en\n",
       "369    pt\n",
       "370    en\n",
       "371    en\n",
       "372    en\n",
       "Name: language, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/affinityanswers-tech/best-ways-of-detecting-language-of-text-in-python-5956846aa5e9\n",
    "# https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language\n",
    "\n",
    "def fast_detect(msg):\n",
    "    try:\n",
    "        ln = model.predict(msg)[0][0].split(\"__\")[2] \n",
    "    except Exception as e:\n",
    "        ln = None\n",
    "    return ln\n",
    "df[\"language\"] = df[\"body\"].apply(fast_detect)\n",
    "df[\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_jmH0hhZb4K",
    "outputId": "04053e4a-6112-4bf7-ac2c-b6ac2785ba44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          en\n",
       "1          en\n",
       "2          en\n",
       "3          en\n",
       "4          en\n",
       "         ... \n",
       "608940     it\n",
       "608941     en\n",
       "608942     en\n",
       "608943     en\n",
       "608944    war\n",
       "Name: language, Length: 608945, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parler_df[\"language\"] = parler_df[\"body\"].apply(fast_detect)\n",
    "parler_df[\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vBwKA4OZb4M",
    "outputId": "05619ce5-97c8-4508-d107-08531fda1c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body createdAtformatted\n",
      "0       week anarchist threatened destroy emancipation...         2020-06-26\n",
      "1       happening now alex jones leading crowd trump s...         2020-11-19\n",
      "2                                 that president one time         2019-06-06\n",
      "3       former michigan trump campaign director scott ...         2020-11-11\n",
      "4       you all think show like recount genuinely inte...         2020-11-21\n",
      "...                                                   ...                ...\n",
      "608940                                              piada         2020-12-09\n",
      "608941  breaking rudy giuliani dominion whistle blower...         2020-11-11\n",
      "608942  imagine trying potus face attack insidious lef...         2020-11-25\n",
      "608943  together now together now aaaaaaaaaaaaaaaaaaaa...         2020-01-12\n",
      "608944                                   hahaha rest piss         2020-12-13\n",
      "\n",
      "[608945 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'language'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-7817a449c682>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'language'"
     ]
    }
   ],
   "source": [
    "print(parler_df)\n",
    "\n",
    "parler_df.drop(parler_df[parler_df['language'] != 'en'].index, inplace=True)\n",
    "\n",
    "print(parler_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKQ9GGviZb4N"
   },
   "outputs": [],
   "source": [
    "# df = parler_df['createdAtformatted'].\n",
    "# parler_df.drop(parler_df[parler_df['language'] != 'en'].index, inplace=True)\n",
    "df = parler_df[parler_df['language'] != 'en'].index)\n",
    "df\n",
    "\n",
    "\n",
    "df = parler_df[parler_df['createdAtformatted'].str.split(n = 0, expand = False).str[0] != 'en'].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-PF-KrRZb4O",
    "outputId": "be663c72-0d19-44be-fc14-442952112383",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'createdAtformatted'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-22da26418f82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#ian_2021 = ( (df['createdAtformatted'].str.split('-'))[0] == '2021' & (df['createdAtformatted'].str.split('-'))[1] == '01')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnov_2020\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'createdAtformatted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2020'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m               \u001b[1;31m# & ((df['createdAtformatted'].str.split('-'))[1] == '11') )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mnov_2020\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4404\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4405\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'createdAtformatted'"
     ]
    }
   ],
   "source": [
    "df = parler_df['createdAtformatted'][0:10]\n",
    "\n",
    "string = \"2020-11-20\"\n",
    "\n",
    "#nov_2020 = ( (df['createdAtformatted'].str.split('-'))[0] == '2020' & (df['createdAtformatted'].str.split('-'))[1] == '11')\n",
    "#dec_2020 = ( (df['createdAtformatted'].str.split('-'))[0] == '2020' & (df['createdAtformatted'].str.split('-'))[1] == '12')\n",
    "#ian_2021 = ( (df['createdAtformatted'].str.split('-'))[0] == '2021' & (df['createdAtformatted'].str.split('-'))[1] == '01')\n",
    "\n",
    "nov_2020 = ( df['createdAtformatted'].map((str.split('-'))[0] == '2020') )\n",
    "              # & ((df['createdAtformatted'].str.split('-'))[1] == '11') )\n",
    "nov_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucwaUAmpZb4Q"
   },
   "outputs": [],
   "source": [
    "typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj9FMIpcZb4Q",
    "outputId": "bd398b60-8642-4e7a-eab6-14c5c9a7059c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body createdAtformatted\n",
      "0  week anarchist threatened destroy emancipation...         2020-06-26\n",
      "1  happening now alex jones leading crowd trump s...         2020-11-19\n",
      "2                            that president one time         2019-06-06\n",
      "3  former michigan trump campaign director scott ...         2020-11-11\n",
      "4  you all think show like recount genuinely inte...         2020-11-21\n",
      "5  newsmax host rob schmidt attorney sidney powel...         2020-11-23\n",
      "6                                                fbi         2020-12-03\n",
      "7  exminneapolis officer involved floyd death ask...         2020-08-30\n",
      "8  michael flynn tweet jeremiah pardon they fight...         2020-11-26\n",
      "9  see this clear evidence vote flipping dominion...         2020-11-22\n",
      "                                                body createdAtformatted  \\\n",
      "0  week anarchist threatened destroy emancipation...         2020-06-26   \n",
      "1  happening now alex jones leading crowd trump s...         2020-11-19   \n",
      "2                            that president one time         2019-06-06   \n",
      "3  former michigan trump campaign director scott ...         2020-11-11   \n",
      "4  you all think show like recount genuinely inte...         2020-11-21   \n",
      "5  newsmax host rob schmidt attorney sidney powel...         2020-11-23   \n",
      "6                                                fbi         2020-12-03   \n",
      "7  exminneapolis officer involved floyd death ask...         2020-08-30   \n",
      "8  michael flynn tweet jeremiah pardon they fight...         2020-11-26   \n",
      "9  see this clear evidence vote flipping dominion...         2020-11-22   \n",
      "\n",
      "   appropiateDate  \n",
      "0           False  \n",
      "1            True  \n",
      "2           False  \n",
      "3            True  \n",
      "4            True  \n",
      "5            True  \n",
      "6            True  \n",
      "7           False  \n",
      "8            True  \n",
      "9            True  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   body                10 non-null     object\n",
      " 1   createdAtformatted  10 non-null     object\n",
      " 2   appropiateDate      10 non-null     bool  \n",
      "dtypes: bool(1), object(2)\n",
      "memory usage: 298.0+ bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-99b50b48dfa9>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['appropiateDate'] = df['createdAtformatted'].apply(check_date)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>createdAtformatted</th>\n",
       "      <th>appropiateDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happening now alex jones leading crowd trump s...</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>former michigan trump campaign director scott ...</td>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you all think show like recount genuinely inte...</td>\n",
       "      <td>2020-11-21</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>newsmax host rob schmidt attorney sidney powel...</td>\n",
       "      <td>2020-11-23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fbi</td>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>michael flynn tweet jeremiah pardon they fight...</td>\n",
       "      <td>2020-11-26</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>see this clear evidence vote flipping dominion...</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body createdAtformatted  \\\n",
       "1  happening now alex jones leading crowd trump s...         2020-11-19   \n",
       "3  former michigan trump campaign director scott ...         2020-11-11   \n",
       "4  you all think show like recount genuinely inte...         2020-11-21   \n",
       "5  newsmax host rob schmidt attorney sidney powel...         2020-11-23   \n",
       "6                                                fbi         2020-12-03   \n",
       "8  michael flynn tweet jeremiah pardon they fight...         2020-11-26   \n",
       "9  see this clear evidence vote flipping dominion...         2020-11-22   \n",
       "\n",
       "   appropiateDate  \n",
       "1            True  \n",
       "3            True  \n",
       "4            True  \n",
       "5            True  \n",
       "6            True  \n",
       "8            True  \n",
       "9            True  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_date(string):\n",
    "    if ( (((string.split('-'))[0] == '2020') and ((string.split('-'))[1] == '11'))          # November 2020\n",
    "        or (((string.split('-'))[0] == '2020') and ((string.split('-'))[1] == '12'))        # December 2020\n",
    "        or (((string.split('-'))[0] == '2021') and ((string.split('-'))[1] == '01')) ):     # January  2021\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "string = \"2021-01-11\"\n",
    "checkDate(string)\n",
    "\n",
    "df = parler_df[0:10]\n",
    "print(df)\n",
    "df['appropiateDate'] = df['createdAtformatted'].apply(check_date)\n",
    "print(df)\n",
    "df.info()\n",
    "\n",
    "df = df[df['appropiateDate'] == True]\n",
    "# df = df.drop(df[df['appropiateDate'] == False])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lVdMoOFZb4S",
    "outputId": "a689430f-bd27-43d4-c160-cc7d02854172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df[df['createdAtformatted'].str.split('-'))[0]) == ]\n",
    "\n",
    "# df2 = df['createdAtformatted'].str.split(n = 0, expand = False).str[0]\n",
    "# df2\n",
    "\n",
    "string = \"2020-07-20\"\n",
    "print((string.split('-'))[0])\n",
    "list = (string.split('-'))\n",
    "#list[0]\n",
    "#list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZb9kbe2Zb4T",
    "outputId": "b7df168d-1a0e-41d7-e080-57ba11bd91cb"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-07541fd15803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dimension of dataframe before: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparler_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mparler_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# 4. Filter out tweets not in relevant time period = dimension reduction (rows) -> only Nov 2020, Dec 2020 and Jan 2021 left\n",
    "\n",
    "def check_date(string):\n",
    "    if ( (((string.split('-'))[0] == '2020') and ((string.split('-'))[1] == '11'))          # November 2020\n",
    "        or (((string.split('-'))[0] == '2020') and ((string.split('-'))[1] == '12'))        # December 2020\n",
    "        or (((string.split('-'))[0] == '2021') and ((string.split('-'))[1] == '01')) ):     # January  2021\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print('Dimension of dataframe before: ' + str(parler_df.shape)) \n",
    "parler_df\n",
    "\n",
    "parler_df['appropiateDate'] = parler_df['createdAtformatted'].apply(check_date)\n",
    "# parler_df.drop(parler_df[parler_df['appropiateDate'] == False].index, inplace=True)\n",
    "parler_df = parler_df[parler_df['appropiateDate'] == True]\n",
    "parler_df.drop(['appropiateDate'], inplace = True, axis = 1)          \n",
    "\n",
    "print('Dimension of dataframe with tweets from relevant time period only: ' + str(parler_df.shape)) \n",
    "parler_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgQqG4rnZb4V",
    "outputId": "372258c8-5551-45a9-9d6f-7155997f5e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body createdAtformatted\n",
      "0                       professor like pot head gone meth         2020-12-13\n",
      "1                                    need spread news ann         2020-12-10\n",
      "2                        fuck yes fire loser non american         2020-11-12\n",
      "3                                          hang pedophile         2020-12-25\n",
      "4       ivanka must sex love deprived human being ever...         2020-12-08\n",
      "...                                                   ...                ...\n",
      "286564  ugly alcoholic sit bar keep picking last guy t...         2020-12-23\n",
      "286565                                               boom         2020-11-02\n",
      "286566  exactly think every time see john robert smirk...         2020-12-25\n",
      "286567                                     twat term twat         2020-11-18\n",
      "286568                                               love         2020-12-20\n",
      "\n",
      "[286569 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "2                        fuck yes fire loser non american         2020-11-12\n",
      "5                                      redparty send info         2020-11-25\n",
      "7       jenelleeason better not tell commie trash frie...         2020-11-15\n",
      "8       metamorphys take blocking win closed minded he...         2020-11-07\n",
      "10              metroswift mama nazi whore that paid xbox         2020-11-24\n",
      "...                                                   ...                ...\n",
      "286558                      fredo fredo fredo broke heart         2020-11-13\n",
      "286561              today november still feel slow reason         2020-11-07\n",
      "286563                                      report parler         2020-11-21\n",
      "286565                                               boom         2020-11-02\n",
      "286567                                     twat term twat         2020-11-18\n",
      "\n",
      "[164948 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "0                       professor like pot head gone meth         2020-12-13\n",
      "1                                    need spread news ann         2020-12-10\n",
      "3                                          hang pedophile         2020-12-25\n",
      "4       ivanka must sex love deprived human being ever...         2020-12-08\n",
      "6                                       cjsteeler disable         2020-12-18\n",
      "...                                                   ...                ...\n",
      "286559  think cheating democrat crazy judge idea messi...         2020-12-12\n",
      "286562  fauciisafraud arrestfauci fauciisatraitor figh...         2020-12-20\n",
      "286564  ugly alcoholic sit bar keep picking last guy t...         2020-12-23\n",
      "286566  exactly think every time see john robert smirk...         2020-12-25\n",
      "286568                                               love         2020-12-20\n",
      "\n",
      "[93916 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "18                                             biden pedo         2021-01-07\n",
      "58      qanonsense know shock not need government look...         2021-01-02\n",
      "64                       sjwhunter fucked should have cap         2021-01-08\n",
      "81                        presidentbiden life bring pussy         2021-01-01\n",
      "83                                               snailman         2021-01-05\n",
      "...                                                   ...                ...\n",
      "286504         thelaststronghold antizionist business you         2021-01-02\n",
      "286539  dang moron notice picked older people alone ne...         2021-01-02\n",
      "286541                         watch want info whatstrump         2021-01-07\n",
      "286556                                  evil vile monster         2021-01-09\n",
      "286560                        remove treasonous snake den         2021-01-04\n",
      "\n",
      "[27705 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cosmi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\cosmi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "C:\\Users\\cosmi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parler_df = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_030_dates.csv')\n",
    "parler_df\n",
    "\n",
    "\n",
    "# 19. Save pandas dataframes for each month after preprocessing without index as CSV file\n",
    "\n",
    "def check_month(date):\n",
    "    if (((date.split('-'))[0] == '2020') and ((date.split('-'))[1] == '11')):  # November 2020\n",
    "        return 'nov'\n",
    "    elif (((date.split('-'))[0] == '2020') and ((date.split('-'))[1] == '12')):  # December 2020\n",
    "        return 'dec'\n",
    "    else:\n",
    "        return 'ian' # January  2021\n",
    "       \n",
    "\n",
    "# print('Dimension of dataframe before: ' + str(parler_df.shape)) \n",
    "print(parler_df)\n",
    "\n",
    "parler_df['month'] = parler_df['createdAtformatted'].apply(check_month)\n",
    "parler_df_nov = parler_df[parler_df['month'] == 'nov']\n",
    "parler_df_nov.drop(['month'], inplace = True, axis = 1) \n",
    "print(parler_df_nov)\n",
    "parler_df_dec = parler_df[parler_df['month'] == 'dec']\n",
    "parler_df_dec.drop(['month'], inplace = True, axis = 1)   \n",
    "print(parler_df_dec)\n",
    "parler_df_ian = parler_df[parler_df['month'] == 'ian']  \n",
    "parler_df_ian.drop(['month'], inplace = True, axis = 1) \n",
    "print(parler_df_ian)\n",
    "\n",
    "# parler_df_nov.to_csv('parler_df_030_dates_nov.csv', index=False)\n",
    "# parler_df_dec.to_csv('parler_df_030_dates_dec.csv', index=False)\n",
    "# parler_df_ian.to_csv('parler_df_030_dates_ian.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emNXLk_zZg0Q"
   },
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# read json file as dataframe\n",
    "parler_df_1 = pd.read_json('D:\\\\bachelors_thesis\\Datasets\\parler_data\\parler_data000000000001.ndjson', lines = True) \n",
    "\n",
    "print(parler_df_1.columns)\n",
    "\n",
    "# remove columns  \n",
    "\n",
    "print(parler_df_1.columns)\n",
    "print('Dimension of whole dataframe: ' + str(parler_df_1.shape)) # df.shape -> (rows, columns)\n",
    "\n",
    "# final_df_1 = pd.DataFrame()\n",
    "# final_df_1['body'] = df['body'].copy()\n",
    "# final_df_1['createdAtformatted'] = df['createdAtformatted'].copy()\n",
    "parler_df_1.drop(parler_df_1.iloc[:, 2:38], inplace = True, axis = 1) # remove all columns between column index 2 to 38\n",
    "parler_df_1.drop(['comments'], inplace = True, axis = 1) # remove first column\n",
    "\n",
    "print(parler_df_1.columns)\n",
    "print('Dimension of final dataframe: ' + str(parler_df_1.shape))\n",
    "\n",
    "# filtering out null values\n",
    "\n",
    "print('Dimension of final whole dataframe: ' + str(parler_df_1.shape)) \n",
    "print(parler_df_1['body'])\n",
    "\n",
    "parler_df_1['body'].replace(\"\", np.nan, inplace=True)\n",
    "parler_df_1.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "print('Dimension of final dataframe after preprocessing: ' + str(parler_df_1.shape)) \n",
    "\n",
    "print(parler_df_1['body'])\n",
    "\n",
    "\n",
    "\n",
    "# 2. Prerequisites â€“ Download nltk stopwords and spacy model\n",
    "\n",
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en\n",
    "\n",
    "# 3. Import Packages\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "import pyLDAvis.gensim # !!! name changed from gensim to gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# 5. Prepare Stopwords\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# 7. Remove emails and newline characters\n",
    "\n",
    "# Convert to list\n",
    "data = parler_df_1.body.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])\n",
    "\n",
    "# 8. Tokenize words and Clean-up text\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n",
    "\n",
    "# 9. Creating Bigram and Trigram Models\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n",
    "# 10. Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en \n",
    "# !!! As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full pipeline package name 'en_core_web_sm' instead.\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "# 11. Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "id2word[0]\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "# 12. Building the Topic Model\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# Build LDA model\n",
    "@jit\n",
    "def LDA():\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "LDA()\n",
    "\n",
    "# 13. View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "# 14. Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# 15. Visualize the topics-keywords\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word) # gensim_models instead od gensim https://github.com/bmabey/pyLDAvis/issues/131\n",
    "vis\n",
    "\n",
    "\n",
    "\n",
    "# Essentials\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datapane as dp\n",
    "#dp.login(token='INSERT_TOKEN_HERE')\n",
    "# Gensim and LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "# NLP stuff\n",
    "import contractions\n",
    "import demoji\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "# Plotting tools\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Miscellaneous\n",
    "from sklearn.manifold import TSNE\n",
    "from pprint import pprint\n",
    "\n",
    "def preprocess(text_col):\n",
    "    \"\"\"This function will apply NLP preprocessing lambda functions over a pandas series such as df['text'].\n",
    "       These functions include converting text to lowercase, removing emojis, expanding contractions, removing punctuation,\n",
    "       removing numbers, removing stopwords, lemmatization, etc.\"\"\"\n",
    "    \n",
    "    # convert to lowercase\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # remove emojis\n",
    "    text_col = text_col.apply(lambda x: demoji.replace(x, \"\"))\n",
    "    \n",
    "    # expand contractions  \n",
    "    text_col = text_col.apply(lambda x: ' '.join([contractions.fix(word) for word in x.split()]))\n",
    "\n",
    "    # remove punctuation\n",
    "    text_col = text_col.apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove numbers\n",
    "    text_col = text_col.apply(lambda x: ' '.join(re.sub(\"[^a-zA-Z]+\", \" \", x).split()))\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = [sw for sw in nltk.corpus.stopwords.words('english') if sw not in ['not', 'no']]\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))\n",
    "\n",
    "    # lemmatization\n",
    "    text_col = text_col.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "    # remove short words\n",
    "    text_col = text_col.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "    return text_col\n",
    "\n",
    "print(parler_df_1['body'])\n",
    "\n",
    "preprocess(parler_df_1['body'])\n",
    "\n",
    "print(parler_df_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl718YDRZhzi"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "pwd\n",
    "\n",
    "import pandas as pd\n",
    "parler = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/parler_df_1.csv.gsheet\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OWxGbPVZp9v"
   },
   "outputs": [],
   "source": [
    "worksheet = gc.open('My cool spreadsheet').sheet1\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "print(rows)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_records(rows)\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# setup\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
    "\n",
    "# read data and put it in a dataframe\n",
    "gsheets = gc.open_by_url('your-link')\n",
    "sheets = gsheets.worksheet('data available').get_all_values()\n",
    "df = pd.DataFrame(sheets[1:], columns=sheets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4I3vtUdZt37"
   },
   "outputs": [],
   "source": [
    "# Load CSV file into Colab from Google Drive via PyDrive\n",
    "\n",
    "# Code to read csv file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEgjReIdZwj0"
   },
   "outputs": [],
   "source": [
    "# link = 'https://drive.google.com/file/d/1KiYk09VqGI6tjNpalom5wI90GrC2p-lz/view'\n",
    "link = 'https://drive.google.com/drive/folders/1ckzXSuPKGqpxR3tVgN_gsnlkdvCbFAQD/view'\n",
    " \n",
    "import pandas as pd\n",
    " \n",
    "# to get the id part of the file\n",
    "id = link.split(\"/\")[-2]\n",
    "print(id)\n",
    " \n",
    " \n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('parler_df_1.csv', mimetype=None) \n",
    "\n",
    "#df = pd.read_csv('parler_df_1.csv')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdFm6BslZ0N7"
   },
   "outputs": [],
   "source": [
    " # Get shareable link\n",
    "link = 'https://docs.google.com/spreadsheets/d/1nQqJTcTY3_7UxKtMfjNrok4echeMuFQ26TGuOyw8KaA/edit?usp=sharing'\n",
    "#fluff, id = link.split('=')\n",
    "fluff, id = link.split('d/')\n",
    "id, fluff = id.split('/edit')\n",
    "print(id)\n",
    "# print (id) # Verify that you have everything after '='\n",
    "\n",
    "downloaded = drive.CreateFile({'id':id})\n",
    "downloaded.GetContentFile('parler_df_1.csv')  \n",
    "parler_df = pd.read_csv('parler_df_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj7Iz7Q2a45H"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/colab-and-google-sheets-surprisingly-powerful-combination-for-data-science-part-1-bbbb11cbd8e\n",
    "# https://colab.research.google.com/notebooks/io.ipynb#scrollTo=J4QxBareshEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    body createdAtformatted\n",
      "0                                             biden pedo         2021-01-07\n",
      "1      qanonsense know shock not need government look...         2021-01-02\n",
      "2                       sjwhunter fucked should have cap         2021-01-08\n",
      "3                        presidentbiden life bring pussy         2021-01-01\n",
      "4                                               snailman         2021-01-05\n",
      "...                                                  ...                ...\n",
      "27700         thelaststronghold antizionist business you         2021-01-02\n",
      "27701  dang moron notice picked older people alone ne...         2021-01-02\n",
      "27702                         watch want info whatstrump         2021-01-07\n",
      "27703                                  evil vile monster         2021-01-09\n",
      "27704                        remove treasonous snake den         2021-01-04\n",
      "\n",
      "[27705 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                            theloudesttrumpet deflected         2021-01-05\n",
      "1      greatestpresident christian fag pedo love prep...         2021-01-03\n",
      "2                 rforester everywhere part life anymore         2021-01-09\n",
      "3      conservatismissociopathy disgusting jerkoff ty...         2021-01-05\n",
      "4      encepthor hehe jealous what lonely feminist ca...         2021-01-01\n",
      "...                                                  ...                ...\n",
      "27614      yes sickens watch news better resource ignore         2021-01-08\n",
      "27615                               awful not surprising         2021-01-01\n",
      "27616                                         creepy joe         2021-01-03\n",
      "27617  rothschild right trump going destroy nwo gooda...         2021-01-03\n",
      "27618                     atifafa biden democratic party         2021-01-07\n",
      "\n",
      "[27619 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0      franciscojguerra indeed happening since civil ...         2021-01-04\n",
      "1                                  hey thanks compliment         2021-01-08\n",
      "2      jimyg fact even know gender besides someone wa...         2021-01-06\n",
      "3                                  danlandry not problem         2021-01-08\n",
      "4                    dirtymike many know complicit hmmmm         2021-01-04\n",
      "...                                                  ...                ...\n",
      "27552                     izal would awesome paint house         2021-01-04\n",
      "27553  johnqgalt let hope time finger crossedguns oil...         2021-01-03\n",
      "27554           say education last lifetime who counting         2021-01-01\n",
      "27555                                          make sick         2021-01-02\n",
      "27556  time put pedophile entire family prison actual...         2021-01-02\n",
      "\n",
      "[27557 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                      absofreakinglutely tear shit down         2021-01-03\n",
      "1      repericswalwell see you want fuck huh bro know...         2021-01-07\n",
      "2       kteff nutshell people making got fucked tax deal         2021-01-02\n",
      "3        piercedxangel give address offered afraid bitch         2021-01-06\n",
      "4      godchild still talking not life thought decide...         2021-01-07\n",
      "...                                                  ...                ...\n",
      "27681                         mustwatchvideo second know         2021-01-02\n",
      "27682                                    petition recall         2021-01-01\n",
      "27683                             incompetent idiot fool         2021-01-05\n",
      "27684                           mob bos trafficking cult         2021-01-02\n",
      "27685  did not believe watch video president obama te...         2021-01-07\n",
      "\n",
      "[27686 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "parler_df_030_jan = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_030_dates_jan.csv')\n",
    "print(parler_df_030_jan)\n",
    "\n",
    "parler_df_031_jan = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_031_dates_jan.csv')\n",
    "print(parler_df_031_jan)\n",
    "\n",
    "parler_df_040_jan = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_040_dates_jan.csv')\n",
    "print(parler_df_040_jan)\n",
    "\n",
    "parler_df_041_jan = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_041_dates_jan.csv')\n",
    "print(parler_df_041_jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    body createdAtformatted\n",
      "0                                             biden pedo         2021-01-07\n",
      "1      qanonsense know shock not need government look...         2021-01-02\n",
      "2                       sjwhunter fucked should have cap         2021-01-08\n",
      "3                        presidentbiden life bring pussy         2021-01-01\n",
      "4                                               snailman         2021-01-05\n",
      "...                                                  ...                ...\n",
      "24995                                    sting operation         2021-01-08\n",
      "24996  would not hire anyone worked big tech would no...         2021-01-10\n",
      "24997  new year day fri january delta force raided bi...         2021-01-06\n",
      "24998       disagree many church failed judgement coming         2021-01-08\n",
      "24999  gaelectionfraud georgia dominion gabesterling ...         2021-01-06\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                            theloudesttrumpet deflected         2021-01-05\n",
      "1      greatestpresident christian fag pedo love prep...         2021-01-03\n",
      "2                 rforester everywhere part life anymore         2021-01-09\n",
      "3      conservatismissociopathy disgusting jerkoff ty...         2021-01-05\n",
      "4      encepthor hehe jealous what lonely feminist ca...         2021-01-01\n",
      "...                                                  ...                ...\n",
      "24995                  pelosi idiot old hag need removed         2021-01-08\n",
      "24996                                      buckle coming         2021-01-02\n",
      "24997  agree this however always one idiot maybe anti...         2021-01-02\n",
      "24998                                             always         2021-01-08\n",
      "24999  one watching either not blame him happened ele...         2021-01-08\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0      franciscojguerra indeed happening since civil ...         2021-01-04\n",
      "1                                  hey thanks compliment         2021-01-08\n",
      "2      jimyg fact even know gender besides someone wa...         2021-01-06\n",
      "3                                  danlandry not problem         2021-01-08\n",
      "4                    dirtymike many know complicit hmmmm         2021-01-04\n",
      "...                                                  ...                ...\n",
      "24995   twitter discriminating president trump supporter         2021-01-09\n",
      "24996                                        trust flynn         2021-01-09\n",
      "24997                                  thanks ride steve         2021-01-01\n",
      "24998                                let delete congress         2021-01-05\n",
      "24999  beginning think acab punk might something prot...         2021-01-06\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                      absofreakinglutely tear shit down         2021-01-03\n",
      "1      repericswalwell see you want fuck huh bro know...         2021-01-07\n",
      "2       kteff nutshell people making got fucked tax deal         2021-01-02\n",
      "3        piercedxangel give address offered afraid bitch         2021-01-06\n",
      "4      godchild still talking not life thought decide...         2021-01-07\n",
      "...                                                  ...                ...\n",
      "24995                                             antifa         2021-01-08\n",
      "24996                  socialist make case socialist son         2021-01-09\n",
      "24997  home not safe denounce fire law enforcement de...         2021-01-06\n",
      "24998                        she proven again she insane         2021-01-04\n",
      "24999                                          true word         2021-01-09\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "parler_df_030_jan = parler_df_030_jan[0:25000]\n",
    "print(parler_df_030_jan)\n",
    "\n",
    "parler_df_031_jan = parler_df_031_jan[0:25000]\n",
    "print(parler_df_031_jan)\n",
    "\n",
    "parler_df_040_jan = parler_df_040_jan[0:25000]\n",
    "print(parler_df_040_jan)\n",
    "\n",
    "parler_df_041_jan = parler_df_041_jan[0:25000]\n",
    "print(parler_df_041_jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>createdAtformatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biden pedo</td>\n",
       "      <td>2021-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qanonsense know shock not need government look...</td>\n",
       "      <td>2021-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sjwhunter fucked should have cap</td>\n",
       "      <td>2021-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidentbiden life bring pussy</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snailman</td>\n",
       "      <td>2021-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>antifa</td>\n",
       "      <td>2021-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>socialist make case socialist son</td>\n",
       "      <td>2021-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>home not safe denounce fire law enforcement de...</td>\n",
       "      <td>2021-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>she proven again she insane</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>true word</td>\n",
       "      <td>2021-01-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body createdAtformatted\n",
       "0                                             biden pedo         2021-01-07\n",
       "1      qanonsense know shock not need government look...         2021-01-02\n",
       "2                       sjwhunter fucked should have cap         2021-01-08\n",
       "3                        presidentbiden life bring pussy         2021-01-01\n",
       "4                                               snailman         2021-01-05\n",
       "...                                                  ...                ...\n",
       "99995                                             antifa         2021-01-08\n",
       "99996                  socialist make case socialist son         2021-01-09\n",
       "99997  home not safe denounce fire law enforcement de...         2021-01-06\n",
       "99998                        she proven again she insane         2021-01-04\n",
       "99999                                          true word         2021-01-09\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parler_df_jan_100000 = pd.concat([parler_df_030_jan, parler_df_031_jan, parler_df_040_jan, parler_df_041_jan], ignore_index=True)\n",
    "parler_df_jan_100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df_jan_100000.to_csv('parler_df_jan_100000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    body createdAtformatted\n",
      "0                      professor like pot head gone meth         2020-12-13\n",
      "1                                   need spread news ann         2020-12-10\n",
      "2                                         hang pedophile         2020-12-25\n",
      "3      ivanka must sex love deprived human being ever...         2020-12-08\n",
      "4                                      cjsteeler disable         2020-12-18\n",
      "...                                                  ...                ...\n",
      "93911  think cheating democrat crazy judge idea messi...         2020-12-12\n",
      "93912  fauciisafraud arrestfauci fauciisatraitor figh...         2020-12-20\n",
      "93913  ugly alcoholic sit bar keep picking last guy t...         2020-12-23\n",
      "93914  exactly think every time see john robert smirk...         2020-12-25\n",
      "93915                                               love         2020-12-20\n",
      "\n",
      "[93916 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                                   spread support trump         2020-12-12\n",
      "1                         pamelacarola not buying moment         2020-12-24\n",
      "2                                karllwilson much handle         2020-12-08\n",
      "3      donaldflunk husband physician idiot wear mask ...         2020-12-08\n",
      "4                            hellofromtheswamp cuck twat         2020-12-30\n",
      "...                                                  ...                ...\n",
      "93778                                       voice reason         2020-12-23\n",
      "93779                          stay course keep fighting         2020-12-18\n",
      "93780  also happened print ballot coincidence like na...         2020-12-27\n",
      "93781                                    echo everywhere         2020-12-08\n",
      "93782                                   arrest prosecute         2020-12-05\n",
      "\n",
      "[93783 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                                             truth hurt         2020-12-18\n",
      "1            hell one traitorous rat bastard goin prison         2020-12-01\n",
      "2                         thank sooooo muck senator cruz         2020-12-25\n",
      "3      jimmychacko going blog bread made rigatoni mea...         2020-12-05\n",
      "4      longhairedbikerguy read people write demonize ...         2020-12-10\n",
      "...                                                  ...                ...\n",
      "93950  sound like someone not want investigator looki...         2020-12-16\n",
      "93951                                    trump president         2020-12-08\n",
      "93952                                                gwb         2020-12-01\n",
      "93953  where full blooded american vigilante sniper t...         2020-12-09\n",
      "93954                                           not ever         2020-12-24\n",
      "\n",
      "[93955 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0      massive big find really dig incriminating evid...         2020-12-23\n",
      "1                                    javaraz neanderthal         2020-12-16\n",
      "2                  snailman not fuck bro talk like nigga         2020-12-18\n",
      "3      jarthur thrown due lack evidence outstanding c...         2020-12-30\n",
      "4                          nottakingmyguns totally agree         2020-12-04\n",
      "...                                                  ...                ...\n",
      "93838  yay ted proud you especially since texas voted...         2020-12-07\n",
      "93839  beautiful wife currently living free country t...         2020-12-15\n",
      "93840                                 reckoning approach         2020-12-05\n",
      "93841  fuck piece shit fuck anyone disrespect country...         2020-12-24\n",
      "93842                                  ready first truth         2020-12-20\n",
      "\n",
      "[93843 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "parler_df_030_dec = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_030_dates_dec.csv')\n",
    "print(parler_df_030_dec)\n",
    "\n",
    "parler_df_031_dec = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_031_dates_dec.csv')\n",
    "print(parler_df_031_dec)\n",
    "\n",
    "parler_df_040_dec = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_040_dates_dec.csv')\n",
    "print(parler_df_040_dec)\n",
    "\n",
    "parler_df_041_dec = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_041_dates_dec.csv')\n",
    "print(parler_df_041_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    body createdAtformatted\n",
      "0                      professor like pot head gone meth         2020-12-13\n",
      "1                                   need spread news ann         2020-12-10\n",
      "2                                         hang pedophile         2020-12-25\n",
      "3      ivanka must sex love deprived human being ever...         2020-12-08\n",
      "4                                      cjsteeler disable         2020-12-18\n",
      "...                                                  ...                ...\n",
      "89995                                           wenbnwen         2020-12-06\n",
      "89996                 morris yeah realized actually done         2020-12-24\n",
      "89997  shewheels comment directed wood response state...         2020-12-26\n",
      "89998                                    bokubux blocked         2020-12-20\n",
      "89999                          hope something going done         2020-12-28\n",
      "\n",
      "[90000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                                   spread support trump         2020-12-12\n",
      "1                         pamelacarola not buying moment         2020-12-24\n",
      "2                                karllwilson much handle         2020-12-08\n",
      "3      donaldflunk husband physician idiot wear mask ...         2020-12-08\n",
      "4                            hellofromtheswamp cuck twat         2020-12-30\n",
      "...                                                  ...                ...\n",
      "89995                 appointed democrat appointed obama         2020-12-07\n",
      "89996  wondered penny also decided cannot speak inten...         2020-12-26\n",
      "89997  karenarmbruster funnyi kind sentiment rarely m...         2020-12-17\n",
      "89998  tabbycat parameter pandemic meet guideline clo...         2020-12-28\n",
      "89999                     thebiffster really van episode         2020-12-07\n",
      "\n",
      "[90000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0                                             truth hurt         2020-12-18\n",
      "1            hell one traitorous rat bastard goin prison         2020-12-01\n",
      "2                         thank sooooo muck senator cruz         2020-12-25\n",
      "3      jimmychacko going blog bread made rigatoni mea...         2020-12-05\n",
      "4      longhairedbikerguy read people write demonize ...         2020-12-10\n",
      "...                                                  ...                ...\n",
      "89995                    god help let history made again         2020-12-21\n",
      "89996                      bakulaholic gotcha make sense         2020-12-10\n",
      "89997                                            lol lol         2020-12-03\n",
      "89998  seems though politician deliberately sticking ...         2020-12-24\n",
      "89999  jules even repuke leader know over crawl back ...         2020-12-20\n",
      "\n",
      "[90000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0      massive big find really dig incriminating evid...         2020-12-23\n",
      "1                                    javaraz neanderthal         2020-12-16\n",
      "2                  snailman not fuck bro talk like nigga         2020-12-18\n",
      "3      jarthur thrown due lack evidence outstanding c...         2020-12-30\n",
      "4                          nottakingmyguns totally agree         2020-12-04\n",
      "...                                                  ...                ...\n",
      "89995  carolwynham know dad abortion depopulation dem...         2020-12-22\n",
      "89996                                                say         2020-12-19\n",
      "89997  say boycott every freaking sport football base...         2020-12-08\n",
      "89998                                  deepstateoperator         2020-12-20\n",
      "89999                                         know right         2020-12-09\n",
      "\n",
      "[90000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "parler_df_030_dec = parler_df_030_dec[0:90000]\n",
    "print(parler_df_030_dec)\n",
    "\n",
    "parler_df_031_dec = parler_df_031_dec[0:90000]\n",
    "print(parler_df_031_dec)\n",
    "\n",
    "parler_df_040_dec = parler_df_040_dec[0:90000]\n",
    "print(parler_df_040_dec)\n",
    "\n",
    "parler_df_041_dec = parler_df_041_dec[0:90000]\n",
    "print(parler_df_041_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>createdAtformatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>professor like pot head gone meth</td>\n",
       "      <td>2020-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need spread news ann</td>\n",
       "      <td>2020-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hang pedophile</td>\n",
       "      <td>2020-12-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivanka must sex love deprived human being ever...</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cjsteeler disable</td>\n",
       "      <td>2020-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359995</th>\n",
       "      <td>carolwynham know dad abortion depopulation dem...</td>\n",
       "      <td>2020-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359996</th>\n",
       "      <td>say</td>\n",
       "      <td>2020-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359997</th>\n",
       "      <td>say boycott every freaking sport football base...</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359998</th>\n",
       "      <td>deepstateoperator</td>\n",
       "      <td>2020-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359999</th>\n",
       "      <td>know right</td>\n",
       "      <td>2020-12-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body createdAtformatted\n",
       "0                       professor like pot head gone meth         2020-12-13\n",
       "1                                    need spread news ann         2020-12-10\n",
       "2                                          hang pedophile         2020-12-25\n",
       "3       ivanka must sex love deprived human being ever...         2020-12-08\n",
       "4                                       cjsteeler disable         2020-12-18\n",
       "...                                                   ...                ...\n",
       "359995  carolwynham know dad abortion depopulation dem...         2020-12-22\n",
       "359996                                                say         2020-12-19\n",
       "359997  say boycott every freaking sport football base...         2020-12-08\n",
       "359998                                  deepstateoperator         2020-12-20\n",
       "359999                                         know right         2020-12-09\n",
       "\n",
       "[360000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parler_df_dec_360000 = pd.concat([parler_df_030_dec, parler_df_031_dec, parler_df_040_dec, parler_df_041_dec], ignore_index=True)\n",
    "parler_df_dec_360000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df_dec_360000.to_csv('parler_df_dec_360000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body createdAtformatted\n",
      "0                        fuck yes fire loser non american         2020-11-12\n",
      "1                                      redparty send info         2020-11-25\n",
      "2       jenelleeason better not tell commie trash frie...         2020-11-15\n",
      "3       metamorphys take blocking win closed minded he...         2020-11-07\n",
      "4               metroswift mama nazi whore that paid xbox         2020-11-24\n",
      "...                                                   ...                ...\n",
      "164943                      fredo fredo fredo broke heart         2020-11-13\n",
      "164944              today november still feel slow reason         2020-11-07\n",
      "164945                                      report parler         2020-11-21\n",
      "164946                                               boom         2020-11-02\n",
      "164947                                     twat term twat         2020-11-18\n",
      "\n",
      "[164948 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "0                                  sgtbear not old enough         2020-11-19\n",
      "1           pnmuntergok pedofile something leave kid this         2020-11-14\n",
      "2       redneckloving respect english language people ...         2020-11-06\n",
      "3       deepwebman relax danny even enunch like find b...         2020-11-24\n",
      "4                                                hlmc yes         2020-11-09\n",
      "...                                                   ...                ...\n",
      "165735                                               amen         2020-11-29\n",
      "165736                                god bless help heal         2020-11-18\n",
      "165737                    look thing might want wait duty         2020-11-20\n",
      "165738  maybe erect statue sidney over girl take prisoner         2020-11-16\n",
      "165739                          especially like young one         2020-11-20\n",
      "\n",
      "[165740 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "0       tired threatened libtards know they are chicke...         2020-11-25\n",
      "1                                   think would like side         2020-11-21\n",
      "2       patriotliberal see difference not russian hoax...         2020-11-19\n",
      "3                     stilsken fuckin rainbow anyway shit         2020-11-25\n",
      "4       biblebytch get ready cry four year dumb fuck a...         2020-11-20\n",
      "...                                                   ...                ...\n",
      "165074                                   need swat teamed         2020-11-22\n",
      "165075                                                wow         2020-11-13\n",
      "165076  sweet treat keep popping day day supported inc...         2020-11-17\n",
      "165077                                        please echo         2020-11-12\n",
      "165078                                      ummm word one         2020-11-19\n",
      "\n",
      "[165079 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "0       joebwinner correct literally conservative burn...         2020-11-24\n",
      "1          factsrus guess that cia operative killed china         2020-11-30\n",
      "2                                   jenniewrennnn welcome         2020-11-01\n",
      "3                                         trumppin boomer         2020-11-23\n",
      "4       aintchaprecious captain debating team talk int...         2020-11-20\n",
      "...                                                   ...                ...\n",
      "164703                   devil worshipper michaelmichelle         2020-11-21\n",
      "164704  obviously buffoon idea electoral process work ...         2020-11-12\n",
      "164705                                    great awakening         2020-11-02\n",
      "164706                                               need         2020-11-26\n",
      "164707  thank ricky may loose good liberal hollywood s...         2020-11-21\n",
      "\n",
      "[164708 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "parler_df_030_nov = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_030_dates_nov.csv')\n",
    "print(parler_df_030_nov)\n",
    "\n",
    "parler_df_031_nov = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_031_dates_nov.csv')\n",
    "print(parler_df_031_nov)\n",
    "\n",
    "parler_df_040_nov = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_040_dates_nov.csv')\n",
    "print(parler_df_040_nov)\n",
    "\n",
    "parler_df_041_nov = pd.read_csv('C:\\\\Users\\\\cosmi\\\\Desktop\\\\ANDREEA\\\\bachelors-thesis\\\\parler_df_041_dates_nov.csv')\n",
    "print(parler_df_041_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    body createdAtformatted\n",
      "0                       fuck yes fire loser non american         2020-11-12\n",
      "1                                     redparty send info         2020-11-25\n",
      "2      jenelleeason better not tell commie trash frie...         2020-11-15\n",
      "3      metamorphys take blocking win closed minded he...         2020-11-07\n",
      "4              metroswift mama nazi whore that paid xbox         2020-11-24\n",
      "...                                                  ...                ...\n",
      "74995                                  gosh that awesome         2020-11-14\n",
      "74996  not count mail back like family crime infested...         2020-11-12\n",
      "74997  went browser phone matter not even using phone...         2020-11-08\n",
      "74998  could peoplewant destroy usa democracy lockdow...         2020-11-12\n",
      "74999  not watch fox not watch fox anymore keep good ...         2020-11-07\n",
      "\n",
      "[75000 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "75000                                      want fair vote         2020-11-11\n",
      "75001                   never let take genocide goy again         2020-11-18\n",
      "75002       people must punished severely none dare again         2020-11-27\n",
      "75003                sorry rating going plummetno foxnews         2020-11-09\n",
      "75004   glad see parler free speech actually alive wel...         2020-11-18\n",
      "...                                                   ...                ...\n",
      "149995                                              truth         2020-11-09\n",
      "149996                           tedcruz repkevinmccarthy         2020-11-06\n",
      "149997                                             agreed         2020-11-17\n",
      "149998                                  god bless linwood         2020-11-30\n",
      "149999  know support you succeed god bless happy thank...         2020-11-27\n",
      "\n",
      "[75000 rows x 2 columns]\n",
      "                                                    body createdAtformatted\n",
      "0      tired threatened libtards know they are chicke...         2020-11-25\n",
      "1                                  think would like side         2020-11-21\n",
      "2      patriotliberal see difference not russian hoax...         2020-11-19\n",
      "3                    stilsken fuckin rainbow anyway shit         2020-11-25\n",
      "4      biblebytch get ready cry four year dumb fuck a...         2020-11-20\n",
      "...                                                  ...                ...\n",
      "74995                        sharing made crap like this         2020-11-06\n",
      "74996  glad see parler free speech actually alive wel...         2020-11-14\n",
      "74997  welcome parler hope enjoy new found freedom fu...         2020-11-27\n",
      "74998          hanging chad dimple sharpy democrat cheat         2020-11-07\n",
      "74999       welcome great you follow favorite commentary         2020-11-09\n",
      "\n",
      "[75000 rows x 2 columns]\n",
      "                                                     body createdAtformatted\n",
      "75000   confused look twitter trump win pensylvania bi...         2020-11-05\n",
      "75001               following you friend tims friend mine         2020-11-08\n",
      "75002                     watched noticeablyuncomfortable         2020-11-09\n",
      "75003           fair stupid primary language possibly one         2020-11-09\n",
      "75004          going real hard put boot far taste leather         2020-11-25\n",
      "...                                                   ...                ...\n",
      "149995             love rudy grateful america know smooch         2020-11-07\n",
      "149996             least australia taking action come man         2020-11-29\n",
      "149997  not recounted also audited counted confirm aut...         2020-11-08\n",
      "149998                                               true         2020-11-12\n",
      "149999                                                yep         2020-11-14\n",
      "\n",
      "[75000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "parler_df_030_nov = parler_df_030_nov[0:75000]\n",
    "print(parler_df_030_nov)\n",
    "\n",
    "parler_df_031_nov = parler_df_031_nov[75000:150000]\n",
    "print(parler_df_031_nov)\n",
    "\n",
    "parler_df_040_nov = parler_df_040_nov[0:75000]\n",
    "print(parler_df_040_nov)\n",
    "\n",
    "parler_df_041_nov = parler_df_041_nov[75000:150000]\n",
    "print(parler_df_041_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>createdAtformatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuck yes fire loser non american</td>\n",
       "      <td>2020-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>redparty send info</td>\n",
       "      <td>2020-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jenelleeason better not tell commie trash frie...</td>\n",
       "      <td>2020-11-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metamorphys take blocking win closed minded he...</td>\n",
       "      <td>2020-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metroswift mama nazi whore that paid xbox</td>\n",
       "      <td>2020-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>love rudy grateful america know smooch</td>\n",
       "      <td>2020-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>least australia taking action come man</td>\n",
       "      <td>2020-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>not recounted also audited counted confirm aut...</td>\n",
       "      <td>2020-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>true</td>\n",
       "      <td>2020-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>yep</td>\n",
       "      <td>2020-11-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body createdAtformatted\n",
       "0                        fuck yes fire loser non american         2020-11-12\n",
       "1                                      redparty send info         2020-11-25\n",
       "2       jenelleeason better not tell commie trash frie...         2020-11-15\n",
       "3       metamorphys take blocking win closed minded he...         2020-11-07\n",
       "4               metroswift mama nazi whore that paid xbox         2020-11-24\n",
       "...                                                   ...                ...\n",
       "299995             love rudy grateful america know smooch         2020-11-07\n",
       "299996             least australia taking action come man         2020-11-29\n",
       "299997  not recounted also audited counted confirm aut...         2020-11-08\n",
       "299998                                               true         2020-11-12\n",
       "299999                                                yep         2020-11-14\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parler_df_nov_300000 = pd.concat([parler_df_030_nov, parler_df_031_nov, parler_df_040_nov, parler_df_041_nov], ignore_index=True)\n",
    "parler_df_nov_300000"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Trial_And_Error.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
